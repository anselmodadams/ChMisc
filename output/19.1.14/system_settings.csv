"ch_version","name","value","changed","description"
"19.1.14","min_compress_block_size","65536",0,"The actual size of the block to compress, if the uncompressed data less than max_compress_block_size is no less than this value and no less than the volume of data for one mark."
"19.1.14","max_compress_block_size","1048576",0,"The maximum size of blocks of uncompressed data before compressing for writing to a table."
"19.1.14","max_block_size","65536",0,"Maximum block size for reading"
"19.1.14","max_insert_block_size","1048576",0,"The maximum block size for insertion, if we control the creation of blocks for insertion."
"19.1.14","min_insert_block_size_rows","1048576",0,"Squash blocks passed to INSERT query to specified size in rows, if blocks are not big enough."
"19.1.14","min_insert_block_size_bytes","268435456",0,"Squash blocks passed to INSERT query to specified size in bytes, if blocks are not big enough."
"19.1.14","max_threads","4",0,"The maximum number of threads to execute the request. By default, it is determined automatically."
"19.1.14","max_read_buffer_size","1048576",0,"The maximum size of the buffer to read from the filesystem."
"19.1.14","max_distributed_connections","1024",0,"The maximum number of connections for distributed processing of one query (should be greater than max_threads)."
"19.1.14","max_query_size","262144",0,"Which part of the query can be read into RAM for parsing (the remaining data for INSERT, if any, is read later)"
"19.1.14","interactive_delay","100000",0,"The interval in microseconds to check if the request is cancelled, and to send progress info."
"19.1.14","connect_timeout","10",0,"Connection timeout if there are no replicas."
"19.1.14","connect_timeout_with_failover_ms","50",0,"Connection timeout for selecting first healthy replica."
"19.1.14","receive_timeout","300",0,""
"19.1.14","send_timeout","300",0,""
"19.1.14","tcp_keep_alive_timeout","0",0,""
"19.1.14","queue_max_wait_ms","5000",0,"The wait time in the request queue, if the number of concurrent requests exceeds the maximum."
"19.1.14","poll_interval","10",0,"Block at the query wait loop on the server for the specified number of seconds."
"19.1.14","distributed_connections_pool_size","1024",0,"Maximum number of connections with one remote server in the pool."
"19.1.14","connections_with_failover_max_tries","3",0,"The maximum number of attempts to connect to replicas."
"19.1.14","extremes","0",0,"Calculate minimums and maximums of the result columns. They can be output in JSON-formats."
"19.1.14","use_uncompressed_cache","0",1,"Whether to use the cache of uncompressed blocks."
"19.1.14","replace_running_query","0",0,"Whether the running request should be canceled with the same id as the new one."
"19.1.14","background_pool_size","16",0,"Number of threads performing background work for tables (for example, merging in merge tree). Only has meaning at server startup."
"19.1.14","background_schedule_pool_size","16",0,"Number of threads performing background tasks for replicated tables. Only has meaning at server startup."
"19.1.14","distributed_directory_monitor_sleep_time_ms","100",0,"Sleep time for StorageDistributed DirectoryMonitors in case there is no work or exception has been thrown."
"19.1.14","distributed_directory_monitor_batch_inserts","0",0,"Should StorageDistributed DirectoryMonitors try to batch individual inserts into bigger ones."
"19.1.14","optimize_move_to_prewhere","1",0,"Allows disabling WHERE to PREWHERE optimization in SELECT queries from MergeTree."
"19.1.14","replication_alter_partitions_sync","1",0,"Wait for actions to manipulate the partitions. 0 - do not wait, 1 - wait for execution only of itself, 2 - wait for everyone."
"19.1.14","replication_alter_columns_timeout","60",0,"Wait for actions to change the table structure within the specified number of seconds. 0 - wait unlimited time."
"19.1.14","load_balancing","random",1,"Which replicas (among healthy replicas) to preferably send a query to (on the first attempt) for distributed processing."
"19.1.14","totals_mode","after_having_exclusive",0,"How to calculate TOTALS when HAVING is present, as well as when max_rows_to_group_by and group_by_overflow_mode = ‘any’ are present."
"19.1.14","totals_auto_threshold","0.5",0,"The threshold for totals_mode = 'auto'."
"19.1.14","compile","0",0,"Whether query compilation is enabled."
"19.1.14","compile_expressions","0",0,"Compile some scalar functions and operators to native code."
"19.1.14","min_count_to_compile","3",0,"The number of structurally identical queries before they are compiled."
"19.1.14","min_count_to_compile_expression","3",0,"The number of identical expressions before they are JIT-compiled"
"19.1.14","group_by_two_level_threshold","100000",0,"From what number of keys, a two-level aggregation starts. 0 - the threshold is not set."
"19.1.14","group_by_two_level_threshold_bytes","100000000",0,"From what size of the aggregation state in bytes, a two-level aggregation begins to be used. 0 - the threshold is not set. Two-level aggregation is used when at least one of the thresholds is triggered."
"19.1.14","distributed_aggregation_memory_efficient","0",0,"Is the memory-saving mode of distributed aggregation enabled."
"19.1.14","aggregation_memory_efficient_merge_threads","0",0,"Number of threads to use for merge intermediate aggregation results in memory efficient mode. When bigger, then more memory is consumed. 0 means - same as 'max_threads'."
"19.1.14","max_parallel_replicas","1",0,"The maximum number of replicas of each shard used when the query is executed. For consistency (to get different parts of the same partition), this option only works for the specified sampling key. The lag of the replicas is not controlled."
"19.1.14","parallel_replicas_count","0",0,""
"19.1.14","parallel_replica_offset","0",0,""
"19.1.14","skip_unavailable_shards","0",0,"Silently skip unavailable shards."
"19.1.14","distributed_group_by_no_merge","0",0,"Do not merge aggregation states from different servers for distributed query processing - in case it is for certain that there are different keys on different shards."
"19.1.14","optimize_skip_unused_shards","0",0,"Assumes that data is distributed by sharding_key. Optimization to skip unused shards if SELECT query filters by sharding_key."
"19.1.14","merge_tree_min_rows_for_concurrent_read","163840",0,"If at least as many lines are read from one file, the reading can be parallelized."
"19.1.14","merge_tree_min_rows_for_seek","0",0,"You can skip reading more than that number of rows at the price of one seek per file."
"19.1.14","merge_tree_coarse_index_granularity","8",0,"If the index segment can contain the required keys, divide it into as many parts and recursively check them."
"19.1.14","merge_tree_max_rows_to_use_cache","1048576",0,"The maximum number of rows per request, to use the cache of uncompressed data. If the request is large, the cache is not used. (For large queries not to flush out the cache.)"
"19.1.14","merge_tree_uniform_read_distribution","1",0,"Distribute read from MergeTree over threads evenly, ensuring stable average execution time of each thread within one read operation."
"19.1.14","mysql_max_rows_to_insert","65536",0,"The maximum number of rows in MySQL batch insertion of the MySQL storage engine"
"19.1.14","optimize_min_equality_disjunction_chain_length","3",0,"The minimum length of the expression `expr = x1 OR ... expr = xN` for optimization "
"19.1.14","min_bytes_to_use_direct_io","0",0,"The minimum number of bytes for input/output operations is bypassing the page cache. 0 - disabled."
"19.1.14","force_index_by_date","0",0,"Throw an exception if there is a partition key in a table, and it is not used."
"19.1.14","force_primary_key","0",0,"Throw an exception if there is primary key in a table, and it is not used."
"19.1.14","mark_cache_min_lifetime","10000",0,"If the maximum size of mark_cache is exceeded, delete only records older than mark_cache_min_lifetime seconds."
"19.1.14","max_streams_to_max_threads_ratio","1",0,"Allows you to use more sources than the number of threads - to more evenly distribute work across threads. It is assumed that this is a temporary solution, since it will be possible in the future to make the number of sources equal to the number of threads, but for each source to dynamically select available work for itself."
"19.1.14","network_compression_method","LZ4",0,"Allows you to select the method of data compression when writing."
"19.1.14","network_zstd_compression_level","1",0,"Allows you to select the level of ZSTD compression."
"19.1.14","priority","0",0,"Priority of the query. 1 - the highest, higher value - lower priority; 0 - do not use priorities."
"19.1.14","log_queries","0",0,"Log requests and write the log to the system table."
"19.1.14","log_queries_cut_to_length","100000",0,"If query length is greater than specified threshold (in bytes), then cut query when writing to query log. Also limit length of printed query in ordinary text log."
"19.1.14","distributed_product_mode","deny",0,"How are distributed subqueries performed inside IN or JOIN sections?"
"19.1.14","max_concurrent_queries_for_user","0",0,"The maximum number of concurrent requests per user."
"19.1.14","insert_deduplicate","1",0,"For INSERT queries in the replicated table, specifies that deduplication of insertings blocks should be preformed"
"19.1.14","insert_sample_with_metadata","0",0,"For INSERT queries, specifies that the server need to send metadata about column defaults to the client. This will be used to calculate default expressions."
"19.1.14","insert_quorum","0",0,"For INSERT queries in the replicated table, wait writing for the specified number of replicas and linearize the addition of the data. 0 - disabled."
"19.1.14","insert_quorum_timeout","600000",0,""
"19.1.14","select_sequential_consistency","0",0,"For SELECT queries from the replicated table, throw an exception if the replica does not have a chunk written with the quorum; do not read the parts that have not yet been written with the quorum."
"19.1.14","table_function_remote_max_addresses","1000",0,"The maximum number of different shards and the maximum number of replicas of one shard in the `remote` function."
"19.1.14","read_backoff_min_latency_ms","1000",0,"Setting to reduce the number of threads in case of slow reads. Pay attention only to reads that took at least that much time."
"19.1.14","read_backoff_max_throughput","1048576",0,"Settings to reduce the number of threads in case of slow reads. Count events when the read bandwidth is less than that many bytes per second."
"19.1.14","read_backoff_min_interval_between_events_ms","1000",0,"Settings to reduce the number of threads in case of slow reads. Do not pay attention to the event, if the previous one has passed less than a certain amount of time."
"19.1.14","read_backoff_min_events","2",0,"Settings to reduce the number of threads in case of slow reads. The number of events after which the number of threads will be reduced."
"19.1.14","memory_tracker_fault_probability","0",0,"For testing of `exception safety` - throw an exception every time you allocate memory with the specified probability."
"19.1.14","enable_http_compression","0",0,"Compress the result if the client over HTTP said that it understands data compressed by gzip or deflate."
"19.1.14","http_zlib_compression_level","3",0,"Compression level - used if the client on HTTP said that it understands data compressed by gzip or deflate."
"19.1.14","http_native_compression_disable_checksumming_on_decompress","0",0,"If you uncompress the POST data from the client compressed by the native format, do not check the checksum."
"19.1.14","count_distinct_implementation","uniqExact",0,"What aggregate function to use for implementation of count(DISTINCT ...)"
"19.1.14","output_format_write_statistics","1",0,"Write statistics about read rows, bytes, time elapsed in suitable output formats."
"19.1.14","add_http_cors_header","0",0,"Write add http CORS header."
"19.1.14","input_format_skip_unknown_fields","0",0,"Skip columns with unknown names from input data (it works for JSONEachRow and TSKV formats)."
"19.1.14","input_format_import_nested_json","0",0,"Map nested JSON data to nested tables (it works for JSONEachRow format)."
"19.1.14","input_format_values_interpret_expressions","1",0,"For Values format: if field could not be parsed by streaming parser, run SQL parser and try to interpret it as SQL expression."
"19.1.14","output_format_json_quote_64bit_integers","1",0,"Controls quoting of 64-bit integers in JSON output format."
"19.1.14","output_format_json_quote_denormals","0",0,"Enables '+nan', '-nan', '+inf', '-inf' outputs in JSON output format."
"19.1.14","output_format_json_escape_forward_slashes","1",0,"Controls escaping forward slashes for string outputs in JSON output format. This is intended for compatibility with JavaScript. Don't confuse with backslashes that are always escaped."
"19.1.14","output_format_pretty_max_rows","10000",0,"Rows limit for Pretty formats."
"19.1.14","output_format_pretty_max_column_pad_width","250",0,"Maximum width to pad all values in a column in Pretty formats."
"19.1.14","output_format_pretty_color","1",0,"Use ANSI escape sequences to paint colors in Pretty formats"
"19.1.14","use_client_time_zone","0",0,"Use client timezone for interpreting DateTime string values, instead of adopting server timezone."
"19.1.14","send_progress_in_http_headers","0",0,"Send progress notifications using X-ClickHouse-Progress headers. Some clients do not support high amount of HTTP headers (Python requests in particular), so it is disabled by default."
"19.1.14","http_headers_progress_interval_ms","100",0,"Do not send HTTP headers X-ClickHouse-Progress more frequently than at each specified interval."
"19.1.14","fsync_metadata","1",0,"Do fsync after changing metadata for tables and databases (.sql files). Could be disabled in case of poor latency on server with high load of DDL queries and high load of disk subsystem."
"19.1.14","input_format_allow_errors_num","0",0,"Maximum absolute amount of errors while reading text formats (like CSV, TSV). In case of error, if both absolute and relative values are non-zero, and at least absolute or relative amount of errors is lower than corresponding value, will skip until next line and continue."
"19.1.14","input_format_allow_errors_ratio","0",0,"Maximum relative amount of errors while reading text formats (like CSV, TSV). In case of error, if both absolute and relative values are non-zero, and at least absolute or relative amount of errors is lower than corresponding value, will skip until next line and continue."
"19.1.14","join_use_nulls","0",0,"Use NULLs for non-joined rows of outer JOINs. If false, use default value of corresponding columns data type."
"19.1.14","join_default_strictness","ALL",0,"Set default strictness in JOIN query. Possible values: empty string, 'ANY', 'ALL'. If empty, query without strictness will throw exception."
"19.1.14","preferred_block_size_bytes","1000000",0,""
"19.1.14","max_replica_delay_for_distributed_queries","300",0,"If set, distributed queries of Replicated tables will choose servers with replication delay in seconds less than the specified value (not inclusive). Zero means do not take delay into account."
"19.1.14","fallback_to_stale_replicas_for_distributed_queries","1",0,"Suppose max_replica_delay_for_distributed_queries is set and all replicas for the queried table are stale. If this setting is enabled, the query will be performed anyway, otherwise the error will be reported."
"19.1.14","preferred_max_column_in_block_size_bytes","0",0,"Limit on max column size in block while reading. Helps to decrease cache misses count. Should be close to L2 cache size."
"19.1.14","insert_distributed_sync","0",0,"If setting is enabled, insert query into distributed waits until data will be sent to all nodes in cluster."
"19.1.14","insert_distributed_timeout","0",0,"Timeout for insert query into distributed. Setting is used only with insert_distributed_sync enabled. Zero value means no timeout."
"19.1.14","distributed_ddl_task_timeout","180",0,"Timeout for DDL query responses from all hosts in cluster. Negative value means infinite."
"19.1.14","stream_flush_interval_ms","7500",0,"Timeout for flushing data from streaming storages."
"19.1.14","format_schema","",0,"Schema identifier (used by schema-based formats)"
"19.1.14","insert_allow_materialized_columns","0",0,"If setting is enabled, Allow materialized columns in INSERT."
"19.1.14","http_connection_timeout","1",0,"HTTP connection timeout."
"19.1.14","http_send_timeout","1800",0,"HTTP send timeout"
"19.1.14","http_receive_timeout","1800",0,"HTTP receive timeout"
"19.1.14","optimize_throw_if_noop","0",0,"If setting is enabled and OPTIMIZE query didn't actually assign a merge then an explanatory exception is thrown"
"19.1.14","use_index_for_in_with_subqueries","1",0,"Try using an index if there is a subquery or a table expression on the right side of the IN operator."
"19.1.14","empty_result_for_aggregation_by_empty_set","0",0,"Return empty result when aggregating without keys on empty set."
"19.1.14","allow_distributed_ddl","1",0,"If it is set to true, then a user is allowed to executed distributed DDL queries."
"19.1.14","odbc_max_field_size","1024",0,"Max size of filed can be read from ODBC dictionary. Long strings are truncated."
"19.1.14","max_rows_to_read","0",0,"Limit on read rows from the most 'deep' sources. That is, only in the deepest subquery. When reading from a remote server, it is only checked on a remote server."
"19.1.14","max_bytes_to_read","0",0,"Limit on read bytes (after decompression) from the most 'deep' sources. That is, only in the deepest subquery. When reading from a remote server, it is only checked on a remote server."
"19.1.14","read_overflow_mode","throw",0,"What to do when the limit is exceeded."
"19.1.14","max_rows_to_group_by","0",0,""
"19.1.14","group_by_overflow_mode","throw",0,"What to do when the limit is exceeded."
"19.1.14","max_bytes_before_external_group_by","0",0,""
"19.1.14","max_rows_to_sort","0",0,""
"19.1.14","max_bytes_to_sort","0",0,""
"19.1.14","sort_overflow_mode","throw",0,"What to do when the limit is exceeded."
"19.1.14","max_bytes_before_external_sort","0",0,""
"19.1.14","max_bytes_before_remerge_sort","1000000000",0,"In case of ORDER BY with LIMIT, when memory usage is higher than specified threshold, perform additional steps of merging blocks before final merge to keep just top LIMIT rows."
"19.1.14","max_result_rows","0",0,"Limit on result size in rows. Also checked for intermediate data sent from remote servers."
"19.1.14","max_result_bytes","0",0,"Limit on result size in bytes (uncompressed). Also checked for intermediate data sent from remote servers."
"19.1.14","result_overflow_mode","throw",0,"What to do when the limit is exceeded."
"19.1.14","max_execution_time","0",0,""
"19.1.14","timeout_overflow_mode","throw",0,"What to do when the limit is exceeded."
"19.1.14","min_execution_speed","0",0,"In rows per second."
"19.1.14","timeout_before_checking_execution_speed","0",0,"Check that the speed is not too low after the specified time has elapsed."
"19.1.14","max_columns_to_read","0",0,""
"19.1.14","max_temporary_columns","0",0,""
"19.1.14","max_temporary_non_const_columns","0",0,""
"19.1.14","max_subquery_depth","100",0,""
"19.1.14","max_pipeline_depth","1000",0,""
"19.1.14","max_ast_depth","1000",0,"Maximum depth of query syntax tree. Checked after parsing."
"19.1.14","max_ast_elements","50000",0,"Maximum size of query syntax tree in number of nodes. Checked after parsing."
"19.1.14","max_expanded_ast_elements","500000",0,"Maximum size of query syntax tree in number of nodes after expansion of aliases and the asterisk."
"19.1.14","readonly","0",0,"0 - everything is allowed. 1 - only read requests. 2 - only read requests, as well as changing settings, except for the 'readonly' setting."
"19.1.14","max_rows_in_set","0",0,"Maximum size of the set (in number of elements) resulting from the execution of the IN section."
"19.1.14","max_bytes_in_set","0",0,"Maximum size of the set (in bytes in memory) resulting from the execution of the IN section."
"19.1.14","set_overflow_mode","throw",0,"What to do when the limit is exceeded."
"19.1.14","max_rows_in_join","0",0,"Maximum size of the hash table for JOIN (in number of rows)."
"19.1.14","max_bytes_in_join","0",0,"Maximum size of the hash table for JOIN (in number of bytes in memory)."
"19.1.14","join_overflow_mode","throw",0,"What to do when the limit is exceeded."
"19.1.14","max_rows_to_transfer","0",0,"Maximum size (in rows) of the transmitted external table obtained when the GLOBAL IN/JOIN section is executed."
"19.1.14","max_bytes_to_transfer","0",0,"Maximum size (in uncompressed bytes) of the transmitted external table obtained when the GLOBAL IN/JOIN section is executed."
"19.1.14","transfer_overflow_mode","throw",0,"What to do when the limit is exceeded."
"19.1.14","max_rows_in_distinct","0",0,"Maximum number of elements during execution of DISTINCT."
"19.1.14","max_bytes_in_distinct","0",0,"Maximum total size of state (in uncompressed bytes) in memory for the execution of DISTINCT."
"19.1.14","distinct_overflow_mode","throw",0,"What to do when the limit is exceeded."
"19.1.14","max_memory_usage","10000000000",1,"Maximum memory usage for processing of single query. Zero means unlimited."
"19.1.14","max_memory_usage_for_user","0",0,"Maximum memory usage for processing all concurrently running queries for the user. Zero means unlimited."
"19.1.14","max_memory_usage_for_all_queries","0",0,"Maximum memory usage for processing all concurrently running queries on the server. Zero means unlimited."
"19.1.14","max_network_bandwidth","0",0,"The maximum speed of data exchange over the network in bytes per second for a query. Zero means unlimited."
"19.1.14","max_network_bytes","0",0,"The maximum number of bytes (compressed) to receive or transmit over the network for execution of the query."
"19.1.14","max_network_bandwidth_for_user","0",0,"The maximum speed of data exchange over the network in bytes per second for all concurrently running user queries. Zero means unlimited."
"19.1.14","max_network_bandwidth_for_all_users","0",0,"The maximum speed of data exchange over the network in bytes per second for all concurrently running queries. Zero means unlimited."
"19.1.14","format_csv_delimiter",",",0,"The character to be considered as a delimiter in CSV data. If setting with a string, a string has to have a length of 1."
"19.1.14","format_csv_allow_single_quotes","1",0,"If it is set to true, allow strings in single quotes."
"19.1.14","format_csv_allow_double_quotes","1",0,"If it is set to true, allow strings in double quotes."
"19.1.14","date_time_input_format","basic",0,"Method to read DateTime from text input formats. Possible values: 'basic' and 'best_effort'."
"19.1.14","log_profile_events","1",0,"Log query performance statistics into the query_log and query_thread_log."
"19.1.14","log_query_settings","1",0,"Log query settings into the query_log."
"19.1.14","log_query_threads","1",0,"Log query threads into system.query_thread_log table. This setting have effect only when 'log_queries' is true."
"19.1.14","send_logs_level","none",0,"Send server text logs with specified minimum level to client. Valid values: 'trace', 'debug', 'information', 'warning', 'error', 'none'"
"19.1.14","enable_optimize_predicate_expression","0",0,"If it is set to true, optimize predicates to subqueries."
"19.1.14","low_cardinality_max_dictionary_size","8192",0,"Maximum size (in rows) of shared global dictionary for LowCardinality type."
"19.1.14","low_cardinality_use_single_dictionary_for_part","0",0,"LowCardinality type serialization setting. If is true, than will use additional keys when global dictionary overflows. Otherwise, will create several shared dictionaries."
"19.1.14","allow_experimental_low_cardinality_type","0",0,"Allows to create table with LowCardinality types."
"19.1.14","decimal_check_overflow","1",0,"Check overflow of decimal arithmetic/comparison operations"
"19.1.14","prefer_localhost_replica","1",0,"1 - always send query to local replica, if it exists. 0 - choose replica to send query between local and remote ones according to load_balancing"
"19.1.14","max_fetch_partition_retries_count","5",0,"Amount of retries while fetching partition from another host."
"19.1.14","asterisk_left_columns_only","0",0,"If it is set to true, the asterisk only return left of join query."
"19.1.14","http_max_multipart_form_data_size","1073741824",0,"Limit on size of multipart/form-data content. This setting cannot be parsed from URL parameters and should be set in user profile. Note that content is parsed and external tables are created in memory before start of query execution. And this is the only limit that has effect on that stage (limits on max memory usage and max execution time have no effect while reading HTTP form data)."
"19.1.14","calculate_text_stack_trace","1",0,"Calculate text stack trace in case of exceptions during query execution. This is the default. It requires symbol lookups that may slow down fuzzing tests when huge amount of wrong queries are executed. In normal cases you should not disable this option."
"19.1.14","allow_ddl","1",0,"If it is set to true, then a user is allowed to executed DDL queries."
"19.1.14","parallel_view_processing","0",0,"Enables pushing to attached views concurrently instead of sequentially."
"19.1.14","enable_debug_queries","0",0,"Enables debug queries such as AST."
"19.1.14","enable_unaligned_array_join","0",0,"Allow ARRAY JOIN with multiple arrays that have different sizes. When this settings is enabled, arrays will be resized to the longest one."
"19.1.14","low_cardinality_allow_in_native_format","1",0,"Use LowCardinality type in Native format. Otherwise, convert LowCardinality columns to ordinary for select query, and convert ordinary columns to required LowCardinality for insert query."
"19.1.14","allow_experimental_multiple_joins_emulation","0",0,"Emulate multiple joins using subselects"
